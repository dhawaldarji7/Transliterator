{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Machine Transliteration of Named Entities from Hindi to English*\n",
    "***(using Conditional Random Fields and Neural networks)***\n",
    "\n",
    "***Author : Dhawal Darji***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
    "\n",
    "# GPU version\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "# Just CPU\n",
    "#!conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "\n",
    "# uncomment the following to install required libraries on first run\n",
    "\n",
    "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n",
    "#!conda install --yes --prefix {sys.prefix} numpy\n",
    "#!conda install --yes --prefix {sys.prefix} scipy\n",
    "#!conda install --yes --prefix {sys.prefix} pandas\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import einops\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom utils\n",
    "\n",
    "'''Syllabifier takes input as a hindi-english word pair and returns syllabified hindi-english words'''\n",
    "import syllabifier as sb\n",
    "\n",
    "'''dataLoader contains utility functions to load/split/manipulate data'''\n",
    "import dataLoader as dl\n",
    "\n",
    "# Assign gpu if one is available\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU/CUDA available?  True\n",
      "Torch version 1.7.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch test\n",
    "import torch\n",
    "\n",
    "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
    "print(\"Torch version\", torch.__version__)\n",
    "print(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab: {'-p-': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "\n",
      "Hindi Vocab: {'-p-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
     ]
    }
   ],
   "source": [
    "# Creating a English and Hindi Vocabulary\n",
    "enVocab = 'a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z'.split(\",\")\n",
    "hnVocab = [chr(c) for c in range(2304, 2432)]\n",
    "\n",
    "# Creating dictionaries for each vocabulary and assigning a index to each letter\n",
    "# Format = {Letter: Index}\n",
    "pad = '-p-' # Index 0 will be used as a pad\n",
    "enVocabDict = {pad: 0}\n",
    "hnVocabDict = {pad: 0}\n",
    "\n",
    "# English vocab dictionary\n",
    "for index, char in enumerate(enVocab):\n",
    "    enVocabDict[char] = index+1\n",
    "\n",
    "# Hindi vocab dictionary \n",
    "for index, char in enumerate(hnVocab):\n",
    "    hnVocabDict[char] = index+1\n",
    "    \n",
    "print(\"English Vocab:\",enVocabDict)\n",
    "print(\"\\nHindi Vocab:\",hnVocabDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordEncode(word, letterIndex, device = 'cpu'):\n",
    "    '''wordEncode takes a word and encodes it \n",
    "    using the vocabulary dictionary into a encoded sequence'''\n",
    "    wordenc = torch.zeros(len(word)+1, 1, len(letterIndex)).to(device)\n",
    "    \n",
    "    for index, letter in enumerate(word):\n",
    "        position = letterIndex[letter]\n",
    "        wordenc[index][0][position] = 1\n",
    "    padPosition = letterIndex[pad]\n",
    "    wordenc[index+1][0][padPosition] = 1\n",
    "    return wordenc\n",
    "    \n",
    "def getEnc(word, letterIndex, device='cpu'):\n",
    "    '''getEnc takes a word and the vocabulary dictionary and \n",
    "    returns a list of indexes for each letter in the vocab'''\n",
    "    getenc = torch.zeros([len(word)+1, 1], dtype=torch.long).to(device)\n",
    "    \n",
    "    for index, letter in enumerate(word):\n",
    "        position = letterIndex[letter]\n",
    "        getenc[index][0] = position \n",
    "    getenc[index+1][0] = letterIndex[pad]\n",
    "    return getenc  \n",
    "\n",
    "# Max characters for the output\n",
    "MaxOutChars = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    '''Utility class to load the dataset'''\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.enWords, self.hnWords = self.getEH(filename)\n",
    "        \n",
    "        # Shuffle indices to get random samples\n",
    "        self.shuffleIndices = list(range(len(self.enWords)))\n",
    "        random.shuffle(self.shuffleIndices)\n",
    "        self.shuffleStart = 0\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        '''Returns a sample from the data with given index'''\n",
    "        return self.enWords[index], self.hnWords[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''Returns the length of dataset'''\n",
    "        return len(self.hnWords)\n",
    "    \n",
    "    def getEH(self,filename):\n",
    "        '''Returns the source(X) and target(y) arrays for the given dataset '''\n",
    "        enWords = []\n",
    "        hnWords = []\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for d in f:\n",
    "                enWords.append(d.split(\" \")[1].strip(\"\\n\"))\n",
    "                hnWords.append(d.split(\" \")[0])\n",
    "\n",
    "        return enWords, hnWords\n",
    "\n",
    "    def getRandomSample(self):\n",
    "        '''Returns a Random Sample from the dataset'''\n",
    "        index = np.random.randint(len(self.enWords))\n",
    "        return self.__getitem__(index)\n",
    "    \n",
    "    def getBatchSolo(self, batchSize, fromSet):\n",
    "        '''Returns a batch of each(source, target) from data for training'''\n",
    "        shuffleEnd = self.shuffleStart + batchSize\n",
    "        batch = []\n",
    "        if shuffleEnd>=len(self.enWords):\n",
    "            batch = [fromSet[i] for i in self.shuffleIndices[0:shuffleEnd%len(self.enWords)]]\n",
    "            end = len(self.enWords)\n",
    "            \n",
    "        return batch + [fromSet[i] for i in self.shuffleIndices[self.shuffleStart:shuffleEnd]]\n",
    "    \n",
    "    def getBatch(self, batchSize):\n",
    "        '''Returns a combined batch of words for training'''\n",
    "        enBatch = self.getBatchSolo(batchSize, self.enWords)\n",
    "        hnBatch = self.getBatchSolo(batchSize, self.hnWords)\n",
    "        self.shuffleStart += (batchSize+1)\n",
    "        \n",
    "        # Shuffle data after every iteration of training\n",
    "        if self.shuffleStart >= len(self.enWords):\n",
    "            random.shuffle(self.shuffleIndices)\n",
    "            self.shuffleStart = 0\n",
    "            \n",
    "        return enBatch, hnBatch\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted into: Train: 80  Test: 20\n",
      "Size of training data: 30736\n",
      "Size of validation data: 5424\n",
      "Size of test data: 9041\n"
     ]
    }
   ],
   "source": [
    "# splitTxt takes the filename and desired trainingDataSize as input,\n",
    "# and splits the data into training, test and validation(15% of trainData) sets.\n",
    "# Default trainingSize = 80\n",
    "dl.splitTxt('datasetBig.txt')\n",
    "\n",
    "trainDataL = DataLoader('train.txt')\n",
    "testDataL = DataLoader('test.txt')\n",
    "valDataL = DataLoader('val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Encoded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Word: anuttam \n",
      "\n",
      "Encoded Word:\n",
      " tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.]]]) \n",
      "\n",
      "Indexes from Vocab:\n",
      " tensor([[ 6],\n",
      "        [41],\n",
      "        [66],\n",
      "        [37],\n",
      "        [78],\n",
      "        [37],\n",
      "        [47],\n",
      "        [ 0]])\n",
      "\n",
      "Hindi Word: अनुत्तम \n",
      "\n",
      "Encoded Word:\n",
      " tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]) \n",
      "\n",
      "Indexes from Vocab:\n",
      " tensor([[ 1],\n",
      "        [14],\n",
      "        [21],\n",
      "        [20],\n",
      "        [20],\n",
      "        [ 1],\n",
      "        [13],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"English Word:\", trainDataL[0][0],\n",
    "      \"\\n\\nEncoded Word:\\n\", wordEncode(trainDataL[0][1], hnVocabDict),\n",
    "      \"\\n\\nIndexes from Vocab:\\n\", getEnc(trainDataL[0][1], hnVocabDict))\n",
    "\n",
    "\n",
    "print(\"\\nHindi Word:\", trainDataL[0][1],\n",
    "      \"\\n\\nEncoded Word:\\n\",wordEncode(trainDataL[0][0].lower(), enVocabDict),\n",
    "      \"\\n\\nIndexes from Vocab:\\n\", getEnc(trainDataL[0][0].lower(), enVocabDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnEncoderDecoder(nn.Module):\n",
    "    '''A encoder-decoder based RNN Seq2Seq model'''\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize, verbose=False):\n",
    "        \n",
    "        super(rnnEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        \n",
    "        self.encoder = nn.GRU(inputSize, hiddenSize)\n",
    "        self.decoder = nn.GRU(outputSize, hiddenSize)\n",
    "        \n",
    "        self.hidden2out = nn.Linear(hiddenSize, outputSize)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, input_, maxOutChars=MaxOutChars, device='cpu', groundTruth = None):\n",
    "        \n",
    "        ###### Encoder #########\n",
    "        encOut, hiddenOut = self.encoder(input_)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Encoder Input:\", input_.shape)\n",
    "            print(\"Encoder hidden output:\", hiddenOut.shape)\n",
    "            print(\"Encoder Output:\", encOut.shape)\n",
    "            \n",
    "        ###### Decoder ##########\n",
    "        decState = hiddenOut\n",
    "        decInput = torch.zeros(1, 1, self.outputSize).to(device)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Decoder State:\", decState.shape)\n",
    "            print(\"Decoder Input:\", decInput.shape)\n",
    "        \n",
    "        outputF = []\n",
    "            \n",
    "        for i in range(maxOutChars):\n",
    "            \n",
    "            output, decoder = self.decoder(decInput, decState)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Decoder Intermediate Output:\", output.shape)\n",
    "                \n",
    "            output = self.hidden2out(decState)\n",
    "            output = self.softmax(output)\n",
    "            outputF.append(output.view(1, -1))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Decoder Output:\", output.shape)\n",
    "                self.verbose = False\n",
    "                \n",
    "            maxIndex = torch.argmax(output, 2, keepdim=True)\n",
    "            if not groundTruth is None:\n",
    "                maxIndex = groundTruth[i].reshape(1,1,1)\n",
    "            newInput = torch.zeros(output.shape, device=device)\n",
    "            newInput.scatter_(2, maxIndex, 1)\n",
    "            \n",
    "            decInput = newInput.detach()\n",
    "            \n",
    "        return outputF\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq using Recurrent Neural Networks with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnAttentionEncoderDecoder(nn.Module):\n",
    "    '''A encoder-decoder based RNN Seq2Seq model with Attention'''\n",
    "    def __init__(self, inputSize,hiddenSize,outputSize,verbose=False):\n",
    "        \n",
    "        super(rnnAttentionEncoderDecoder,self).__init__()\n",
    "        \n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        \n",
    "        self.encoder = nn.GRU(inputSize, hiddenSize)\n",
    "        self.decoder = nn.GRU(hiddenSize*2, hiddenSize)\n",
    "        \n",
    "        self.hidden2out = nn.Linear(hiddenSize, outputSize)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        self.U = nn.Linear(self.hiddenSize, self.hiddenSize)\n",
    "        self.W = nn.Linear(self.hiddenSize, self.hiddenSize)\n",
    "        \n",
    "        self.attention = nn.Linear(hiddenSize,1) \n",
    "        self.out2hidden = nn.Linear(self.outputSize, self.hiddenSize)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, input_, maxOutChars=MaxOutChars, device='cpu', groundTruth=None):\n",
    "        \n",
    "        ########## encoder ####################\n",
    "        encOut, hidden = self.encoder(input_)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Encoder output',encOut.shape)\n",
    "        \n",
    "        encOut = encOut.view(-1, self.hiddenSize)\n",
    "            \n",
    "        ########### decoder ###################\n",
    "        decState = hidden\n",
    "        decInput = torch.zeros(1, 1, self.outputSize).to(device)\n",
    "        \n",
    "        outputs=[]\n",
    "        U = self.U(encOut)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Decoder state', decState.shape)\n",
    "            print('Decoder input', decInput.shape)\n",
    "            print('U', U.shape)\n",
    "            \n",
    "        for i in range(maxOutChars):\n",
    "            \n",
    "            W = self.W(decState.view(1,-1).repeat(encOut.shape[0],1))\n",
    "            V = self.attention(torch.tanh(U + W))\n",
    "            \n",
    "            attentionWeights = fun.softmax(V.view(1,-1),dim=1)\n",
    "                \n",
    "            attentionApplied = torch.bmm(attentionWeights.unsqueeze(0), encOut.unsqueeze(0))\n",
    "            \n",
    "            embedding = self.out2hidden(decInput)\n",
    "            \n",
    "            decInput = torch.cat((embedding[0], attentionApplied[0]), 1).unsqueeze(0)\n",
    "\n",
    "            out, decState = self.decoder(decInput, decState)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('W',W.shape)\n",
    "                print('V',V.shape)\n",
    "                print('Attention Weights',attentionWeights.shape)\n",
    "                print('Encoder output',encOut.shape)\n",
    "                print('Attention applied',attentionApplied.shape)\n",
    "                print('Decoder input',decInput.shape)\n",
    "                print('Decoder intermediate output',out.shape)\n",
    "                \n",
    "            out = self.hidden2out(decState)\n",
    "            out = self.softmax(out)\n",
    "            outputs.append(out.view(1,-1))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Decoder Output', out.shape)\n",
    "                self.verbose=False\n",
    "                \n",
    "            maxIndex = torch.argmax(out,2,keepdim=True)\n",
    "            \n",
    "            if not groundTruth is None:\n",
    "                maxIndex = groundTruth[i].reshape(1,1,1)\n",
    "                \n",
    "            newInput = torch.zeros(out.shape, device=device)\n",
    "            newInput.scatter_(2, maxIndex, 1)\n",
    "            \n",
    "            decInput = newInput.detach()\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model, word, maxOutChars, device='cpu'):\n",
    "    '''Predicts the transliterated word for given word'''\n",
    "    model.eval().to(device)\n",
    "    sourceEnc = wordEncode(word, hnVocabDict)\n",
    "    prediction = model(sourceEnc, maxOutChars)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Runs for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: torch.Size([8, 1, 129])\n",
      "Encoder hidden output: torch.Size([1, 1, 256])\n",
      "Encoder Output: torch.Size([8, 1, 256])\n",
      "Decoder State: torch.Size([1, 1, 256])\n",
      "Decoder Input: torch.Size([1, 1, 27])\n",
      "Decoder Intermediate Output: torch.Size([1, 1, 256])\n",
      "Decoder Output: torch.Size([1, 1, 27])\n"
     ]
    }
   ],
   "source": [
    "model = rnnEncoderDecoder(len(hnVocabDict),256,len(enVocabDict),verbose=True)\n",
    "prediction = getPrediction(model,trainDataL[0][1],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output torch.Size([8, 1, 256])\n",
      "Decoder state torch.Size([1, 1, 256])\n",
      "Decoder input torch.Size([1, 1, 27])\n",
      "U torch.Size([8, 256])\n",
      "W torch.Size([8, 256])\n",
      "V torch.Size([8, 1])\n",
      "Attention Weights torch.Size([1, 8])\n",
      "Encoder output torch.Size([8, 256])\n",
      "Attention applied torch.Size([1, 1, 256])\n",
      "Decoder input torch.Size([1, 1, 512])\n",
      "Decoder intermediate output torch.Size([1, 1, 256])\n",
      "Decoder Output torch.Size([1, 1, 27])\n"
     ]
    }
   ],
   "source": [
    "model = rnnAttentionEncoderDecoder(len(hnVocabDict),256,len(enVocabDict),verbose=True)\n",
    "prediction = getPrediction(model,trainDataL[0][1],20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchTrain(model, optimizer, criterion, batchSize, device='cpu', teacherForce = False):\n",
    "    '''Trains the model on one batch of data'''\n",
    "    \n",
    "    model.train().to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get a batch of batchSize from the data\n",
    "    enBatch, hnBatch = trainDataL.getBatch(batchSize)\n",
    "    \n",
    "    # Calculate the loss for the run on a single batch\n",
    "    totalLoss = 0\n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        source = wordEncode(hnBatch[i], hnVocabDict, device)\n",
    "        target = getEnc(enBatch[i], enVocabDict, device)\n",
    "        outputs = model(source, target.shape[0], device, groundTruth = target if teacherForce else None)\n",
    "        \n",
    "        for index, output in enumerate(outputs):\n",
    "            loss = criterion(output, target[index])/batchSize\n",
    "            loss.backward(retain_graph = True)\n",
    "            totalLoss+=loss\n",
    "    \n",
    "    # One step of optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return the loss for the current batch\n",
    "    return totalLoss/batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBatchTrain(model, lr=0.1, numBatches=100, batchSize=10, displayFreq=10, device='cpu'):\n",
    "    '''Trains the data on specified number of batches and batchSize'''\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion=nn.NLLLoss(ignore_index=-1)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = opt.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    # teacherForce ratio\n",
    "    teacherForceRatio = numBatches//3\n",
    "    \n",
    "    # loss array\n",
    "    lossArr = np.zeros(numBatches+2)\n",
    "    \n",
    "    # Train the model for numBatches(epochs)\n",
    "    for i in range(numBatches+1):\n",
    "        \n",
    "        lossArr[i+1] = (lossArr[i]*i+batchTrain(model,\n",
    "                                              optimizer,\n",
    "                                              criterion,\n",
    "                                              batchSize,\n",
    "                                              device=device, \n",
    "                                              teacherForce=i<teacherForceRatio))/(i+1)\n",
    "        \n",
    "        # display the loss according to the displayFreq\n",
    "        if i%displayFreq==0 and i!=0:\n",
    "            print('Iteration ',i,' Loss',lossArr[i])\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model,'model.pt')  \n",
    "    \n",
    "    # Return the total training loss\n",
    "    return lossArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  10  Loss 0.10385165363550186\n",
      "Iteration  20  Loss 0.0951119065284729\n",
      "Iteration  30  Loss 0.0907028540968895\n",
      "Iteration  40  Loss 0.08697733283042908\n",
      "Iteration  50  Loss 0.0839361771941185\n",
      "Iteration  60  Loss 0.08151286095380783\n",
      "Iteration  70  Loss 0.07939902693033218\n",
      "Iteration  80  Loss 0.07770224660634995\n",
      "Iteration  90  Loss 0.07626838237047195\n",
      "Iteration  100  Loss 0.07506533712148666\n",
      "Iteration  110  Loss 0.0739450454711914\n",
      "Iteration  120  Loss 0.07313119620084763\n",
      "Iteration  130  Loss 0.07230781763792038\n",
      "Iteration  140  Loss 0.07161548733711243\n",
      "Iteration  150  Loss 0.07104188203811646\n",
      "Iteration  160  Loss 0.07052134722471237\n",
      "Iteration  170  Loss 0.07002943754196167\n",
      "Iteration  180  Loss 0.06962256878614426\n",
      "Iteration  190  Loss 0.06922026723623276\n",
      "Iteration  200  Loss 0.06887208670377731\n",
      "Iteration  210  Loss 0.06854281574487686\n",
      "Iteration  220  Loss 0.06817112863063812\n",
      "Iteration  230  Loss 0.06787997484207153\n",
      "Iteration  240  Loss 0.06763587892055511\n",
      "Iteration  250  Loss 0.06741355359554291\n",
      "Iteration  260  Loss 0.06720246374607086\n",
      "Iteration  270  Loss 0.06698594987392426\n",
      "Iteration  280  Loss 0.06677393615245819\n",
      "Iteration  290  Loss 0.06657344847917557\n",
      "Iteration  300  Loss 0.06640151143074036\n",
      "Iteration  310  Loss 0.06622103601694107\n",
      "Iteration  320  Loss 0.06605907529592514\n",
      "Iteration  330  Loss 0.0659061148762703\n",
      "Iteration  340  Loss 0.0657564103603363\n",
      "Iteration  350  Loss 0.0656050369143486\n",
      "Iteration  360  Loss 0.06547092646360397\n",
      "Iteration  370  Loss 0.06532733887434006\n",
      "Iteration  380  Loss 0.06519991159439087\n",
      "Iteration  390  Loss 0.06507404148578644\n",
      "Iteration  400  Loss 0.06497849524021149\n",
      "Iteration  410  Loss 0.0648839995265007\n",
      "Iteration  420  Loss 0.06480355560779572\n",
      "Iteration  430  Loss 0.06471190601587296\n",
      "Iteration  440  Loss 0.06462030112743378\n",
      "Iteration  450  Loss 0.06452034413814545\n",
      "Iteration  460  Loss 0.0644388273358345\n",
      "Iteration  470  Loss 0.06436452269554138\n",
      "Iteration  480  Loss 0.06427303701639175\n",
      "Iteration  490  Loss 0.06418827176094055\n",
      "Iteration  500  Loss 0.06411603093147278\n",
      "Iteration  510  Loss 0.06405044347047806\n",
      "Iteration  520  Loss 0.06397075951099396\n",
      "Iteration  530  Loss 0.0638989731669426\n",
      "Iteration  540  Loss 0.0638335645198822\n",
      "Iteration  550  Loss 0.06376338005065918\n",
      "Iteration  560  Loss 0.06370269507169724\n",
      "Iteration  570  Loss 0.06364557147026062\n",
      "Iteration  580  Loss 0.06357717514038086\n",
      "Iteration  590  Loss 0.06351769715547562\n",
      "Iteration  600  Loss 0.0634511411190033\n",
      "Iteration  610  Loss 0.06340359151363373\n",
      "Iteration  620  Loss 0.06333578377962112\n",
      "Iteration  630  Loss 0.06329140812158585\n",
      "Iteration  640  Loss 0.0632421225309372\n",
      "Iteration  650  Loss 0.06318794935941696\n",
      "Iteration  660  Loss 0.06312772631645203\n",
      "Iteration  670  Loss 0.06308617442846298\n",
      "Iteration  680  Loss 0.06304172426462173\n",
      "Iteration  690  Loss 0.06300026923418045\n",
      "Iteration  700  Loss 0.06295905262231827\n",
      "Iteration  710  Loss 0.06290890276432037\n",
      "Iteration  720  Loss 0.06285382062196732\n",
      "Iteration  730  Loss 0.0628192350268364\n",
      "Iteration  740  Loss 0.0627862885594368\n",
      "Iteration  750  Loss 0.06273863464593887\n",
      "Iteration  760  Loss 0.06270657479763031\n",
      "Iteration  770  Loss 0.06266926974058151\n",
      "Iteration  780  Loss 0.06262538582086563\n",
      "Iteration  790  Loss 0.06258803606033325\n",
      "Iteration  800  Loss 0.06254655867815018\n",
      "Iteration  810  Loss 0.06251311302185059\n",
      "Iteration  820  Loss 0.062479522079229355\n",
      "Iteration  830  Loss 0.062441635876894\n",
      "Iteration  840  Loss 0.062410879880189896\n",
      "Iteration  850  Loss 0.06237151846289635\n",
      "Iteration  860  Loss 0.06233242526650429\n",
      "Iteration  870  Loss 0.06229517608880997\n",
      "Iteration  880  Loss 0.06226571649312973\n",
      "Iteration  890  Loss 0.062233053147792816\n",
      "Iteration  900  Loss 0.06220337003469467\n",
      "Iteration  910  Loss 0.06217656284570694\n",
      "Iteration  920  Loss 0.06214563176035881\n",
      "Iteration  930  Loss 0.06211201846599579\n",
      "Iteration  940  Loss 0.062087591737508774\n",
      "Iteration  950  Loss 0.06205839291214943\n",
      "Iteration  960  Loss 0.062031339854002\n",
      "Iteration  970  Loss 0.06200431287288666\n",
      "Iteration  980  Loss 0.061981577426195145\n",
      "Iteration  990  Loss 0.06194714456796646\n",
      "Iteration  1000  Loss 0.06192293018102646\n"
     ]
    }
   ],
   "source": [
    "# Training the RNN Seq2Seq model\n",
    "\n",
    "start = datetime.now()\n",
    "model = rnnEncoderDecoder(len(hnVocabDict),512,len(enVocabDict))\n",
    "lossHistory = runBatchTrain(model, lr=0.01, numBatches=1000, batchSize=256, displayFreq=10, device=gpu)\n",
    "end = datetime.now()\n",
    "\n",
    "trainTimeRnn = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  10  Loss 0.10054649412631989\n",
      "Iteration  20  Loss 0.08827856928110123\n",
      "Iteration  30  Loss 0.0818902924656868\n",
      "Iteration  40  Loss 0.07753408700227737\n",
      "Iteration  50  Loss 0.07372581213712692\n",
      "Iteration  60  Loss 0.06974831968545914\n",
      "Iteration  70  Loss 0.06581412255764008\n",
      "Iteration  80  Loss 0.06177011877298355\n",
      "Iteration  90  Loss 0.0577525794506073\n",
      "Iteration  100  Loss 0.053879402577877045\n",
      "Iteration  110  Loss 0.05037663131952286\n",
      "Iteration  120  Loss 0.047205567359924316\n",
      "Iteration  130  Loss 0.04439013451337814\n",
      "Iteration  140  Loss 0.04191342368721962\n",
      "Iteration  150  Loss 0.03980052471160889\n",
      "Iteration  160  Loss 0.03787409886717796\n",
      "Iteration  170  Loss 0.036167118698358536\n",
      "Iteration  180  Loss 0.034679729491472244\n",
      "Iteration  190  Loss 0.033341508358716965\n",
      "Iteration  200  Loss 0.03211692348122597\n",
      "Iteration  210  Loss 0.031018635258078575\n",
      "Iteration  220  Loss 0.02999475970864296\n",
      "Iteration  230  Loss 0.029045648872852325\n",
      "Iteration  240  Loss 0.028170866891741753\n",
      "Iteration  250  Loss 0.02735750935971737\n",
      "Iteration  260  Loss 0.026575952768325806\n",
      "Iteration  270  Loss 0.025846103206276894\n",
      "Iteration  280  Loss 0.025172250345349312\n",
      "Iteration  290  Loss 0.02454693429172039\n",
      "Iteration  300  Loss 0.02395055443048477\n",
      "Iteration  310  Loss 0.023403141647577286\n",
      "Iteration  320  Loss 0.022894537076354027\n",
      "Iteration  330  Loss 0.022416457533836365\n",
      "Iteration  340  Loss 0.02234332263469696\n",
      "Iteration  350  Loss 0.0223283339291811\n",
      "Iteration  360  Loss 0.02227475680410862\n",
      "Iteration  370  Loss 0.02217097021639347\n",
      "Iteration  380  Loss 0.022014964371919632\n",
      "Iteration  390  Loss 0.021868733689188957\n",
      "Iteration  400  Loss 0.021720856428146362\n",
      "Iteration  410  Loss 0.02156505361199379\n",
      "Iteration  420  Loss 0.021407373249530792\n",
      "Iteration  430  Loss 0.02127058058977127\n",
      "Iteration  440  Loss 0.021113162860274315\n",
      "Iteration  450  Loss 0.020985199138522148\n",
      "Iteration  460  Loss 0.02086004987359047\n",
      "Iteration  470  Loss 0.020751578733325005\n",
      "Iteration  480  Loss 0.020628537982702255\n",
      "Iteration  490  Loss 0.02048892341554165\n",
      "Iteration  500  Loss 0.02035696618258953\n",
      "Iteration  510  Loss 0.020225241780281067\n",
      "Iteration  520  Loss 0.020083380863070488\n",
      "Iteration  530  Loss 0.019963180646300316\n",
      "Iteration  540  Loss 0.019849609583616257\n",
      "Iteration  550  Loss 0.019739851355552673\n",
      "Iteration  560  Loss 0.0196444783359766\n",
      "Iteration  570  Loss 0.01954551786184311\n",
      "Iteration  580  Loss 0.019486786797642708\n",
      "Iteration  590  Loss 0.019444754347205162\n",
      "Iteration  600  Loss 0.019391678273677826\n",
      "Iteration  610  Loss 0.019305475056171417\n",
      "Iteration  620  Loss 0.01923941820859909\n",
      "Iteration  630  Loss 0.019164277240633965\n",
      "Iteration  640  Loss 0.01910131424665451\n",
      "Iteration  650  Loss 0.019030127674341202\n",
      "Iteration  660  Loss 0.018960056826472282\n",
      "Iteration  670  Loss 0.01889781840145588\n",
      "Iteration  680  Loss 0.018841072916984558\n",
      "Iteration  690  Loss 0.018768850713968277\n",
      "Iteration  700  Loss 0.018695635721087456\n",
      "Iteration  710  Loss 0.018638521432876587\n",
      "Iteration  720  Loss 0.0185783039778471\n",
      "Iteration  730  Loss 0.01849924959242344\n",
      "Iteration  740  Loss 0.018449142575263977\n",
      "Iteration  750  Loss 0.018402116373181343\n",
      "Iteration  760  Loss 0.01836724393069744\n",
      "Iteration  770  Loss 0.018341680988669395\n",
      "Iteration  780  Loss 0.018320035189390182\n",
      "Iteration  790  Loss 0.018296867609024048\n",
      "Iteration  800  Loss 0.018358398228883743\n",
      "Iteration  810  Loss 0.018599271774291992\n",
      "Iteration  820  Loss 0.019018467515707016\n",
      "Iteration  830  Loss 0.019484298303723335\n",
      "Iteration  840  Loss 0.020154479891061783\n",
      "Iteration  850  Loss 0.021095938980579376\n",
      "Iteration  860  Loss 0.022258223965764046\n",
      "Iteration  870  Loss 0.023190291598439217\n",
      "Iteration  880  Loss 0.023892326280474663\n",
      "Iteration  890  Loss 0.02445693127810955\n",
      "Iteration  900  Loss 0.02492663636803627\n",
      "Iteration  910  Loss 0.025354206562042236\n",
      "Iteration  920  Loss 0.02573867328464985\n",
      "Iteration  930  Loss 0.02609270252287388\n",
      "Iteration  940  Loss 0.026407573372125626\n",
      "Iteration  950  Loss 0.026683663949370384\n",
      "Iteration  960  Loss 0.026943102478981018\n",
      "Iteration  970  Loss 0.027178611606359482\n",
      "Iteration  980  Loss 0.02739516831934452\n",
      "Iteration  990  Loss 0.02758827619254589\n",
      "Iteration  1000  Loss 0.027753641828894615\n"
     ]
    }
   ],
   "source": [
    "# Training the Attention RNN Seq2Seq model\n",
    "\n",
    "start = datetime.now()\n",
    "modelAtt = rnnAttentionEncoderDecoder(len(hnVocabDict),512,len(enVocabDict))\n",
    "lossHistory = runBatchTrain(modelAtt, lr=0.01, numBatches=1000, batchSize=256, displayFreq=10, device=gpu)\n",
    "end = datetime.now()\n",
    "\n",
    "trainTimeAttRnn = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    '''Gives out predictions on whole data with specified model'''\n",
    "    predY = []\n",
    "    for i in range(len(data)):\n",
    "        eng, hindi = data[i]\n",
    "        pred = getPrediction(model, hindi, len(eng))\n",
    "        en = []\n",
    "        for index, p in enumerate(pred):\n",
    "            value, indices = p.topk(1)\n",
    "            en.append(enVocab[indices.tolist()[0][0] - 1])\n",
    "        predY.append(\"\".join(en))\n",
    "    return predY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "def elem_accuracy(y_hat, y_gold):\n",
    "    '''Evaluation measure for predictions of a y-sequence, returns average position-wise matches.'''\n",
    "    if len(y_hat) != len(y_gold):\n",
    "        raise ValueError(len(y_hat),' != ',len(y_gold))\n",
    "    matches = np.sum([y_hat[i]==y_gold[i] for i in range(len(y_gold))])\n",
    "    return 1.0 * matches / len(y_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on train/val/test sets using both the NN models\n",
    "\n",
    "RnnpredYT = predict(trainDataL, model)\n",
    "RnnpredYV = predict(valDataL, model)\n",
    "RnnpredY = predict(testDataL, model)\n",
    "\n",
    "AttRnnpredYT = predict(trainDataL, modelAtt)\n",
    "AttRnnpredYV = predict(valDataL, modelAtt)\n",
    "AttRnnpredY = predict(testDataL, modelAtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for RNN-Seq2Seq: 1:20:31.786652\n",
      "Training time for Attention-RNN-Seq2Seq: 6:33:56.326687\n",
      "\n",
      "Train Accuracy:\n",
      "RNNSeq2Seq: 0.2685235098496182 \n",
      "AttentionRNNSeq2Seq: 0.5670080225270107\n",
      "\n",
      "Train F1 score:\n",
      "RNNSeq2Seq: 0.3027218635348887 \n",
      "AttRNNSeq2Seq: 0.5146689729493704\n",
      "\n",
      "Validation Accuracy:\n",
      "RNNSeq2Seq: 0.2619285840040818 \n",
      "AttentionRNNSeq2Seq: 0.5646673559560728\n",
      "\n",
      "Validation F1 score:\n",
      "RNNSeq2Seq: 0.2944087980021897 \n",
      "AttRNNSeq2Seq: 0.5135938082294154\n",
      "\n",
      "Test Accuracy:\n",
      "RNNSeq2Seq: 0.2606880289091068 \n",
      "AttentionRNNSeq2Seq: 0.5669303035306439\n",
      "\n",
      "Test F1 score:\n",
      "RNNSeq2Seq: 0.2922726427816583 \n",
      "AttRNNSeq2Seq: 0.5145139506703765\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time for RNN-Seq2Seq:\", trainTimeRnn)\n",
    "print(\"Training time for Attention-RNN-Seq2Seq:\", trainTimeAttRnn)\n",
    "\n",
    "trainAccRnn = np.average([elem_accuracy(RnnpredYT[i], trainDataL[i][0]) for i in range(len(trainDataL))])\n",
    "trainAccRnnAtt = np.average([elem_accuracy(AttRnnpredYT[i], trainDataL[i][0]) for i in range(len(trainDataL))])\n",
    "print(\"\\nTrain Accuracy:\\nRNNSeq2Seq:\", trainAccRnn, \"\\nAttentionRNNSeq2Seq:\", trainAccRnnAtt)\n",
    "print(\"\\nTrain F1 score:\\nRNNSeq2Seq:\", metrics.flat_f1_score(RnnpredYT, trainDataL[:][0], average='weighted'),\n",
    "     \"\\nAttRNNSeq2Seq:\", metrics.flat_f1_score(AttRnnpredYT, trainDataL[:][0], average='weighted'))\n",
    "\n",
    "valAccRnn = np.average([elem_accuracy(RnnpredYV[i], valDataL[i][0]) for i in range(len(valDataL))])\n",
    "valAccRnnAtt = np.average([elem_accuracy(AttRnnpredYV[i], valDataL[i][0]) for i in range(len(valDataL))])\n",
    "print(\"\\nValidation Accuracy:\\nRNNSeq2Seq:\", valAccRnn, \"\\nAttentionRNNSeq2Seq:\", valAccRnnAtt)\n",
    "print(\"\\nValidation F1 score:\\nRNNSeq2Seq:\", metrics.flat_f1_score(RnnpredYV, valDataL[:][0], average='weighted'),\n",
    "     \"\\nAttRNNSeq2Seq:\", metrics.flat_f1_score(AttRnnpredYV, valDataL[:][0], average='weighted'))\n",
    "\n",
    "testAccRnn = np.average([elem_accuracy(RnnpredY[i], testDataL[i][0]) for i in range(len(testDataL))])\n",
    "testAccRnnAtt = np.average([elem_accuracy(AttRnnpredY[i], testDataL[i][0]) for i in range(len(testDataL))])\n",
    "print(\"\\nTest Accuracy:\\nRNNSeq2Seq:\", testAccRnn, \"\\nAttentionRNNSeq2Seq:\", testAccRnnAtt)\n",
    "print(\"\\nTest F1 score:\\nRNNSeq2Seq:\", metrics.flat_f1_score(RnnpredY, testDataL[:][0], average='weighted'),\n",
    "     \"\\nAttRNNSeq2Seq:\", metrics.flat_f1_score(AttRnnpredY, testDataL[:][0], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model parameters for RNN Seq2Seq Model:\", sum([param.nelement() for param in model.parameters()]))\n",
    "print(\"Model parameters for Attetnion RNN Seq2Seq Model:\", sum([param.nelement() for param in modelAtt.parameters()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
