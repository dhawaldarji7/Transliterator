{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Machine Transliteration of Named Entities from Hindi to English*\n",
    "***(using Conditional Random Fields and Neural networks)***\n",
    "\n",
    "***Author : Dhawal Darji***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines when you run this for the first time. \n",
    "# It will install necessary libraries into your python environment.\n",
    "# You only need to run it once, then you can comment it out again to save some time.\n",
    "\n",
    "# !pip install indic-transliteration\n",
    "# !pip install sklearn_crfsuite\n",
    "\n",
    "#!conda install --yes --prefix {sys.prefix} numpy\n",
    "#!conda install --yes --prefix {sys.prefix} scipy\n",
    "#!conda install --yes --prefix {sys.prefix} pandas\n",
    "\n",
    "import os\n",
    "\n",
    "# Custom utils\n",
    "\n",
    "'''Syllabifier takes input as a hindi-english word pair and returns syllabified hindi-english words'''\n",
    "import syllabifier as sb\n",
    "\n",
    "'''dataLoader contains utility functions to load/split/manipulate data'''\n",
    "import dataLoader as dl\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from sklearn_crfsuite import CRF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 45201\n",
      "One entry in dataset: विठ्ठल,vitthal\n"
     ]
    }
   ],
   "source": [
    "# loads data from a txt file into a numpy array\n",
    "\n",
    "data = dl.loadTxt('datasetBig.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42935\n",
      "One entry in Syllabified data:\n",
      " [['वि' 'vi']\n",
      " ['ठ्ठ' 'ththa']\n",
      " ['ल' 'l']]\n"
     ]
    }
   ],
   "source": [
    "# Syllabifies the data into hindiSyllabifiedWord, englishSyllabifiedWord\n",
    "\n",
    "data = np.array([d for d in data if ('\\u093c' not in d.split(\",\")[0] and \n",
    "                                     '\\u0949' not in d.split(\",\")[0] and \n",
    "                                     '\\u0911' not in d.split(\",\")[0] and \n",
    "                                    '\\u0945' not in d.split(\",\")[0])])\n",
    "\n",
    "sylData = np.array([sb.syllabify(d.split(\",\")[0], d.split(\",\")[1]) for d in data])\n",
    "print(\"One entry in Syllabified data:\\n\", sylData[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities & Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hindi vowels\n",
    "hVowels = ['\\u0905','\\u0906','\\u0907','\\u0908','\\u0909','\\u090A','\\u090B','\\u090F','\\u0910','\\u0913','\\u0914']\n",
    "\n",
    "# List of hindi consonants\n",
    "hConsonants = [u'\\u0915',u'\\u0916',u'\\u0917',u'\\u0918',u'\\u0919',u'\\u091A',u'\\u091B',u'\\u091C',\n",
    "               u'\\u091D',u'\\u091E',u'\\u091F',u'\\u0920',u'\\u0921',u'\\u0922',u'\\u0923',u'\\u0924',\n",
    "              u'\\u0925',u'\\u0926',u'\\u0927',u'\\u0928',u'\\u092A',u'\\u092B',\n",
    "               u'\\u092C',u'\\u092D',u'\\u092E',u'\\u092F',u'\\u0930',u'\\u0932',u'\\u0933',u'\\u0935',\n",
    "              u'\\u0936',u'\\u0937',u'\\u0938',u'\\u0939']\n",
    "\n",
    "# List of hindi consonants in english\n",
    "eConsonants = 'ka,kha,ga,gha,cha,ja,jha,ta,tha,da,dha,na,pa,pha,ba,bha,ma,ya,ra,la,va,sha,sa,ha'.split(\",\")\n",
    "\n",
    "def wordLen(word):\n",
    "    return range(len(word))\n",
    "\n",
    "def getHindi(word):\n",
    "    '''Returns the Hindi(X) words from data'''\n",
    "    return np.array(word).T[0].T\n",
    "\n",
    "def getEnglish(word):\n",
    "    '''Returns the English(Y) words from data'''\n",
    "    return np.array(word).T[1].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_accuracy(y_hat_l, y_gold):\n",
    "    '''Evaluation measure for predictions of a y-sequence, returns average position-wise matches.'''\n",
    "    y_hat = np.array(y_hat_l)\n",
    "    if len(y_hat) != len(y_gold):\n",
    "        raise ValueError(len(y_hat),' != ',len(y_gold))\n",
    "    matches = np.sum(y_gold == np.array(y_hat))\n",
    "    return 1.0 * matches / len(y_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tags: ['vi', 'ththa', 'l', 'ma', 'ni', 'ka', 'ra', 'v', 'shan', 'ta', 'nu', 'jyo', 'ti', 'an', 'ba', 'r', 'a', 'nan', 'ha', 's', 'la', 'kshma', 'n', 'san', 'pa', 'da', 'va', 'rdha', 'pra', 'sa', 'd', 'shu', 'to', 'he', 'u', 'rda', 'le', 'k', 'e', 'na', 't', 'j~na', 'ne', 'shva', 'go', 'dau', 'jen', 'ghu', 'su', 're', 'no', 'je', 'mi', 'di', 'sha', 'ru', 'chi', 'dha', 'dhu', 'sham', 'm', 'rna', 'mo', 'rcha', 'pu', 'rva', 'ju', 'chan', 'dra', 'kam', 'yo', 'gi', 'rmi', 'bha', 'chai', 'li', 'tra', 'vai', 'y', 'ga', 'ri', 'hu', 'rya', 'tha', 'gha', 'ja', 'man', 'vin', 'tu', 'so', 'ya', 'pri', 'vrri', 'b', 'smi', 'dhi', 'bhi', 'ke', 'me', 'bra', 'mha', 'in', 'rma', 'de', 'tya', 'o', 'on', 'krri', 'shna', 'shri', 'rri', 'shi', 'ran', 'g', 'ai', 'ji', 'pha', 'gho', 'rsha', 'hen', 'sin', 'ro', 'hi', 'gau', 'du', 'pi', 'lu', 'tna', 'sai', 'p', 'j', 'rmen', 'ren', 'ska', 'nyu', 'rju', 'ku', 'yu', 'shthi', 'si', 'ryo', 'dhrri', 'dro', 'khmi', 'pam', 'vya', 'c', 'tru', 'lmi', 'gri', 'kra', 'cha', 'kan', 'bi', 'kshmi', 'din', 'bhan', 'hrri', 'van', 'mu', 'pre', 'ki', 'lo', 'sho', 'nma', 'kta', 'tri', 'kau', 'rdu', 'i', 'ddi', 'gya', 'gdha', 'srri', 'shti', 'shka', 'lha', 'tsa', 'tma', 'gan', 'gu', 'dhe', 'shya', 'jha', 'pan', 'lin', 'sne', 'dre', 'chin', 'jhe', 'bu', 'dhon', 'tte', 'shchan', 'nna', 'ten', 'stu', 'kun', 'gam', 'kha', 'shai', 'dhda', 'lpa', 'bba', 'bdu', 'bbu', 'ko', 'gra', 've', 'she', 'bhu', 'bhya', 'bo', 'chyu', 'dvai', 'gni', 'se', 'te', 'khi', 'kram', 'ksha', 'kshi', 'lla', 'ppa', 'lan', 'lpe', 'len', 'rshu', 'pun', 'tta', 'rga', 'kai', 'shpa', 'shra', 'jva', 'nvi', 'rtha', 'bho', 'sau', 'lya', 'mrri', 'ye', 'dma', 'thu', 'dri', 'kte', 'rti', 'bham', 'sva', 'cham', 'nya', 'jin', 'shmi', 'tyu', 'rin', 'tam', 'am', 'h', 'ram', 'rchi', 'rja', 'rji', 'rne', 'rpa', 'rpi', 'po', 'nji', 'ttha', 'shvi', 'tin', 'yya', 'yyu', 'jhi', 'sla', 'phu', 'ven', 'sta', 'tryan', 'nho', 'mam', 'be', 'sun', 'ttu', 'ban', 'dai', 'bam', 'spa', 'bhai', 'bhau', 'yan', 'shma', 'gin', 'min', 'bhrri', 'jan', 'pe', 'pen', 'pin', 'dyu', 'bin', 'do', 'dhen', 'hma', 'bri', 'nni', 'kre', 'kri', 'chu', 'cho', 'dram', 'jo', 'che', 'dam', 'kku', 'tran', 'tram', 'tre', 'kka', 'dva', 'rten', 'rte', 'kshu', 'bhra', 'lun', 'ge', 'vrrin', 'kshe', 'dan', 'rsa', 'rve', 'pta', 'pti', 'han', 'vam', 'rshi', 'vra', 'rme', 'vyam', 'dho', 'mra', 'ptam', 'pte', 'vyen', 'don', 'rge', 'rje', 'dvi', 'mma', 'pten', 'shre', 'ken', 'phai', 'khri', 'lgu', 'nin', 'phi', 'nen', 'dhva', 'grri', 'gun', 'rke', 'men', 'rsu', 'rsi', 'hri', 'rdi', 'mti', 'sma', 'ham', 'hin', 'nme', 'dru', 'jhu', 'jhan', 'shnu', 'yen', 'thi', 'lpi', 'nha', 'nhai', 'lam', 'rta', 'nai', 'rthi', 'gen', 'khai', 'khan', 'khe', 'khu', 'kru', 'rni', 'shnen', 'bdha', 'tan', 'rtan', 'mai', 'nnu', 'rken', 'mau', 'gham', 'rjha', 'nam', 'tyun', 'khta', 'mun', 'bhen', 'tpa', 'rba', 'rbha', 'rbhi', 'rmo', 'shcha', 'ssi', 'svi', 'dme', 'dmi', 'rvi', 'shen', 'yam', 'pram', 'thvi', 'prri', 'rnen', 'shpe', 'shpi', 'pya', 'lka', 'kti', 'gran', 'au', 'yi', 'lvi', 'kin', 'tnam', 'tni', 'tne', 'rau', 'jja', 'tvi', 'tun', 'ho', 'sam', 'chchi', 'rbe', 'rvam', 'tye', 'bbi', 'den', 'shau', 'shin', 'shlo', 'shram', 'shru', 'shve', 'dda', 'dhde', 'dham', 'sri', 'sto', 'khin', 'sve', 'pna', 'pni', 'sti', 'sya', 'ryam', 'mri', 'lli', 'trri', 'tau', 'yyi', 'rka', 'tva', 'ddha', 'dya', 'ghne', 'gne', 'khya', 'dhya', 'vri', 'vyo', 'j~ne', 'stha', 'tmi', 'bdi', 'phra', 'phro', 'rjo', 'shtha', 'psa', 'rki', 'run', 'rvin', 'bai', 'nno', 'tti', 'shta', 'vyan', 'drau', 'drri', 'phe', 'lki', 'rdhi', 'hai', 'hru', 'chcha', 'psi', 'jam', 'sna', 'rla', 'ltha', 'mya', 'bdhi', 'lau', 'kya', 'shki', 'mon', 'lta', 'nhi', 'rgi', 'vo', 'kki', 'nau', 'nrri', 'pau', 'jya', 'rbi', 'gna', 'vu', 'pro', 'shpam', 'tho', 'kma', 'gve', 'ddhi', 'skrri', 'saun', 'phta', 'rli', 'kla', 'shun', 'ddhe', 'skan', 'smrri', 'sni', 'gda', 'spu', 'rai', 'trin', 'un', 'tka', 'tpre', 'ghi', 'shle', 'the', 'rde', 'kham', 'ben', 'ske', 'ton', 'stri', 'mhai', 'sra', 'dde', 'jhin', 'kho', 'gon', 'bhe', 'kon', 'shte', 'chau', 'rlo', 'phun', 'lle', 'kke', 'pho', 'vha', 'bhon', 'chun', 'ste', 'nhe', 'rha', 'khon', 'nne', 'rhe', 'ghe', 'tki', 'dhye', 'bon', 'tto', 'en', 'sthi', 'rkan', 'thye', 'mba', 'rra', 'trau', 'lga', 'bhin', 'bhain', 'gai', 'daun', 'dhau', 'lhi', 'gva', 'lai', 'sain', 'nnau', 'lti', 'jai', 'jhun', 'jau', 'jham', 'hau', 'rgo', 'thau', 'main', 'jho', 'mho', 'aun', 'pai', 'llo', 'ndi', 'sen', 'ron', 'than', 'jjai', 'kva', 'kaun', 'chon', 'ghon', 'khun', 'then', 'lhe', 'ghun', 'dun', 'kkha', 'phpha', 'nyo', 'msa', 'bau', 'tro', 'jhai', 'gga', 'lon', 'nda', 'dhan', 'phau', 'rno', 'bun', 'rdo', 'nsi', 'llu', 'ssa', 'ndo', 'lva', 'mbha', 'nsa', 'ldi', 'stra', 'ksa', 'dhun', 'thai', 'lmo', 'mpa', 'sko', 'khti', 'tai', 'sba', 'rho', 'nhu', 'son', 'shto', 'ggi', 'mhi', 'phan', 'chen', 'chaun', 'bda', 'thya', 'ppe', 'gaun', 'mmi', 'hon', 'hun', 'vhe', 'rle', 'gre', 'lma', 'rkhe', 'rko', 'ski', 'lsa', 'nun', 'shchi', 'pon', 'paun', 'mta', 'rbhe', 'sno', 'spe', 'thin', 'shrrin', 'lho', 'myu', 'zi', 'phren', 'phlo', 'kvi', 'lain', 'kle', 'hla', '.', '.n', 'za', 'rain', 'byu', 'kain', 'lsi', 'sli', 'rre', 'lyu', 'lve', 'zo', 'ndu', 'qi', 'kyu', 'shphi', 'kve', 'kro', 'bru', '.di', 'lda', 'rtsa', 'phri', 'z', 'bain', 'tsu', 'kyo', 'gru', 'nka', 'stren', 'ple', 'hyu', 'gbi', 'klin', 'spen', 'lhai', 'ze', 'qa', 'ldin', 'stre', 'glai', 'kso', 'nye', 'nve', '.da', 'skai', 'kli', 'ble', 'nta', 'glo', 'zai', 'tain', 'gla', 'klu', 'rlin', 'jyu', 'vhi', 'glen', 'sku', 'ndra', 'vhu', 'qrri', 'hain', 'mbo', 'sbi', 'phram', 'sse', 'khva', 'lbo', 'nte', 'lcha', 'pla', 'zu', 'rse', 'ddu', 'stam', 'lye', 'phli', 'pain', 'mbe', 'mbre', 'bro', 'zon', 'kse', 'kvam', 'tren', 'lba', 'bre', 'vyu', 'prin', 'grai', 'kvai', 'gro', 'gli', 'sprin', 'sle', 'spo', 'grain', 'khen', 'skva', 'bli', 'vva', 'bji', 'scha', '.de', 'von', 'kron', 'svai', 'ktra', 'blo', 'phre', 'slo', 'mpe', 'mmu', 'phti', 'bya', 'lphi', 'bren', 'stai', 'nva', 'ldo', 'chain', 'brai', 'tve', 'plo', 'phran', 'ktro', 'klo', 'pru', 'rghe', 'nde', 'lpha', 'mna', 'mla', 'ksi', 'pyu', 'hva', 'chche', 'non', 'mbu', 'lshi', 'nshi', 'bsa', 'jon', 'mbi', 'krai', 'dhru', 'yon', 'rjen', 'gram', 'rsta', 'ldra', 'shon', 'blu', 'ktri', '.do', 'slin', 'stro', 'gle', 'rkyu', 'zain', 'thyu', 'spi', 'ddo', 'zin', 'zza', 'lkha', 'yun', 'phyu', 'mpri', 'thri', 'vre', 'dyan', 'kto', 'lte', 'jve', 'rto', 'mte', 'trai', 'rtu', 'bhru', 'kko', 'rdra', 'zra', 'jra', 'lbe', 'mme', 'qu', 'phle', 'svin', 'klai', 'phen', 'lbi', 'kvin', 'lko', 'q', 'lja', 'brri', 'gta', 'shtri', 'dyo', 'lde', 'ndre', 'ttau', 'blai', 'fa', 'nvha', 'thra', 'hya', 'rvo', 'mbri', 'bram', 'gyu', 'lji', 'sphi', 'lpho', 'dain', 'chchu', 'shyo', 'ktre', 'lme', 'zma', 'ndri', 'nko', 'phrain', 'kyon', 'nti', 'phain', 'khshi', 'bja', 'rza', 'lphre', 'ain', 'tse', 'sji', 'stru', 'mple', 'rtyu', 'pko', 'gmo', 'pshi', 'mko', 'tchi', 'vba', 'syu', 'bla', 'sten', 'hli', 'lkro']\n",
      "Number of tags: 961\n",
      "Data splitted into: Train: 80  Test: 20\n",
      "Size of training data: 29196\n",
      "Size of validation data: 5152\n",
      "Size of test data: 8587\n"
     ]
    }
   ],
   "source": [
    "# Splits the syllabified data into train/test/validation test\n",
    "\n",
    "import collections\n",
    "\n",
    "# count the number of occurrences of a syllable\n",
    "countList = []\n",
    "for d in sylData:\n",
    "    for s in getEnglish(d):\n",
    "        countList.append(s)\n",
    "\n",
    "countDict = collections.Counter(countList)\n",
    "\n",
    "# select the most frequent syllables as tags\n",
    "tags = [key for key,count in countDict.items() if count > 1]\n",
    "print(\"All tags:\", tags)\n",
    "print(\"Number of tags:\", len(tags))\n",
    "\n",
    "# syllables other than the tags will be marked as 'x'\n",
    "for d in sylData:\n",
    "    for i, s in enumerate(d):\n",
    "        if s[1] not in tags:\n",
    "            d[i][1] = \"x\"\n",
    "\n",
    "# Split the data into train/validation/test set\n",
    "# Validation set is 15% of the train set\n",
    "trainData, valData, testData = dl.splitData(sylData, 80) #(syllabifiedData, trainSize)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the X and Y from data\n",
    "\n",
    "trainX = np.array([getHindi(data) for data in trainData])\n",
    "trainY = np.array([getEnglish(data) for data in trainData])\n",
    "\n",
    "valX = np.array([getHindi(data) for data in valData])\n",
    "valY = np.array([getEnglish(data) for data in valData])\n",
    "\n",
    "testX = np.array([getHindi(data) for data in testData])\n",
    "testY = np.array([getEnglish(data) for data in testData])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn CRF\n",
    "\n",
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'intercept': 1, 'hindi': 'न', 'isVowel': 0, 'isConsonant': 1, 'containsMatra': 0, 'containsHalfConsonant': 0, 'BOW': 1, 'nextHindiSyllable': 'म', 'nextEngSyllable': 'ma', 'nextVowel': 0}\n",
      " {'intercept': 1, 'hindi': 'म', 'isVowel': 0, 'isConsonant': 1, 'containsMatra': 0, 'containsHalfConsonant': 0, 'prevHindiSyllable': 'न', 'prevEngSyllable': 'na', 'prevVowel': 0, 'nextHindiSyllable': 'स्ते', 'nextEngSyllable': 'ste', 'nextVowel': 0}\n",
      " {'intercept': 1, 'hindi': 'स्ते', 'isVowel': 0, 'isConsonant': 0, 'containsMatra': 1, 'containsHalfConsonant': 1, 'prevHindiSyllable': 'म', 'prevEngSyllable': 'ma', 'prevVowel': 0, 'EOW': 1}]\n"
     ]
    }
   ],
   "source": [
    "def syllable2features(word, index):\n",
    "    '''Takes a syllable as input and returns a feature vector'''\n",
    "    \n",
    "    hindiSyllable = getHindi(word)[index]\n",
    "    engSyllable = getEnglish(word)[index]\n",
    "\n",
    "    features = {\n",
    "        'intercept': 1,\n",
    "        'hindi': hindiSyllable,\n",
    "        'isVowel': 1 if hindiSyllable in hVowels else 0,\n",
    "        'isConsonant': 1 if hindiSyllable in hConsonants else 0,\n",
    "        'containsMatra': 1 if hindiSyllable not in hConsonants else 0,\n",
    "        'containsHalfConsonant': 1 if u'\\u094D' in hindiSyllable else 0,  \n",
    "    }\n",
    "    \n",
    "    if index > 0:\n",
    "        prevHindiSyllable = getHindi(word)[index-1]\n",
    "        prevEngSyllable = getEnglish(word)[index-1]\n",
    "        features.update({\n",
    "            'prevHindiSyllable': prevHindiSyllable,\n",
    "            'prevEngSyllable': prevEngSyllable,\n",
    "            'prevVowel': 1 if prevHindiSyllable in hVowels else 0,\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        features['BOW'] = 1\n",
    "        \n",
    "    if index < len(word)-1:\n",
    "        nextHindiSyllable = getHindi(word)[index+1]\n",
    "        nextEngSyllable = getEnglish(word)[index+1]\n",
    "        features.update({\n",
    "            'nextHindiSyllable': nextHindiSyllable,\n",
    "            'nextEngSyllable': nextEngSyllable,\n",
    "            'nextVowel': 1 if nextHindiSyllable in hVowels else 0,\n",
    "        })\n",
    "    else:\n",
    "        features['EOW'] = 1\n",
    "\n",
    "        \n",
    "    return features\n",
    "\n",
    "def word2features(word):\n",
    "    '''takes a whole word as input and returns feature vector for each syllable'''\n",
    "    return np.array([syllable2features(word, index) for index in range(len(word))])\n",
    "\n",
    "# Get the training, validation and test feature vector\n",
    "TrainX = np.array([word2features(w) for w in trainData])\n",
    "ValX = np.array([word2features(w) for w in valData])\n",
    "TestX = np.array([word2features(w) for w in testData])\n",
    "\n",
    "print(word2features([['न' , 'na'],\n",
    "              ['म', 'ma'], \n",
    "              ['स्ते', 'ste']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF, metrics\n",
    "from datetime import datetime\n",
    "\n",
    "# The crf model with the hyperparameters\n",
    "crf = CRF(\n",
    "algorithm='lbfgs',\n",
    "c2=0.1,\n",
    "c1=1.0,\n",
    "max_iterations=100,\n",
    "all_possible_transitions = True)\n",
    "\n",
    "# training!\n",
    "start = datetime.now()\n",
    "crf.fit(TrainX, trainY)\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "\n",
    "# predict on train, val and test sets\n",
    "ypredT = crf.predict(TrainX)\n",
    "ypredV = crf.predict(ValX)\n",
    "ypred = crf.predict(TestX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 8:43:55.616911\n",
      "\n",
      "Train Accuracy: 0.9936271711497769\n",
      "Train F1 score: 0.9957018132202916\n",
      "\n",
      "Validation Accuracy: 0.9871894409937888\n",
      "Validation F1 score: 0.9906058358129142\n",
      "\n",
      "Test Accuracy: 0.98552920527708\n",
      "Test F1 score: 0.9887670197817535\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time:\", (end - start))\n",
    "\n",
    "print(\"\\nTrain Accuracy:\", np.average([elem_accuracy(ypredT[i], trainY[i]) for i in range(len(trainY))]))\n",
    "print(\"Train F1 score:\", metrics.flat_f1_score(ypredT, trainY, average='weighted'))\n",
    "\n",
    "print(\"\\nValidation Accuracy:\", np.average([elem_accuracy(ypredV[i], valY[i]) for i in range(len(valY))]))\n",
    "print(\"Validation F1 score:\", metrics.flat_f1_score(ypredV, valY, average='weighted'))\n",
    "\n",
    "print(\"\\nTest Accuracy:\", np.average([elem_accuracy(ypred[i], testY[i]) for i in range(len(testY))]))\n",
    "print(\"Test F1 score:\", metrics.flat_f1_score(ypred, testY, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CRF\n",
    "\n",
    "This part is potentially broken and doesn't work completely\n",
    "\n",
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.085536923187668\n",
      "[1. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X = 0\n",
    "Y = 1\n",
    "\n",
    "def ks(seq):\n",
    "    '''Returns the range over a seq'''\n",
    "    return range(1, len(seq))\n",
    "\n",
    "def log_factor(w, feats):\n",
    "    '''Returns the factor in log space'''\n",
    "    return np.dot(w, feats)\n",
    "\n",
    "def factor(w, feats):\n",
    "    '''Returns the factor in probability space '''\n",
    "    return np.exp(log_factor(w, feats))\n",
    "\n",
    "def general_features(tagPrev, x_seq, k, ftype=\"uni\"):\n",
    "    '''Return features for a given sequence'''\n",
    "    return features(tagPrev,x_seq[k-1],k,ftype)\n",
    "\n",
    "def general_factor(w,tagPrev, x_seq, k):\n",
    "    '''Return factor for a given sequence'''\n",
    "    return factor(w, general_features(tagPrev, x_seq, k))\n",
    "\n",
    "print(general_factor(np.ones(maxUnif),'ap',trainX[1], 1))\n",
    "print(general_features('ka',trainX[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram features\n",
    "def unigram(tagPrev, word, index):\n",
    "    hindiSyllable = word #[index]\n",
    "    f1 = 1.0\n",
    "    f2 = 1.0 if hindiSyllable in hVowels else 0.0\n",
    "    f3 = 1.0 if hindiSyllable in hConsonants else 0.0\n",
    "    f4 = 1.0 if hindiSyllable not in hConsonants else 0.0 # is the syllable a complete consonant?\n",
    "    f5 = 1.0 if u'\\u094d' in hindiSyllable else 0.0 # does the syllable contain a half consonant?\n",
    "\n",
    "    return np.array([f1, f2, f3, f4, f5])\n",
    "maxUnif = 5\n",
    "\n",
    "# Bigram features\n",
    "def bigram(word, index):\n",
    "    hindiSyllable = word[index]\n",
    "    \n",
    "    f1 = 1.0\n",
    "    f2 = 1.0 if hindiSyllable in hVowels else 0.0\n",
    "    f3 = 1.0 if hindiSyllable in hConsonants else 0.0\n",
    "    f4 = 1.0 if hindiSyllable not in hConsonants else 0.0 # is the syllable a complete consonant?\n",
    "    f5 = 1.0 if u'\\u094d' in hindiSyllable else 0.0 # does the syllable contain a half consonant?\n",
    "    f6 = 1.0 if index > 0 else 0.0 # if index > 0, currY will also depend on prevX, prevY\n",
    "        \n",
    "    return np.array([f1, f2, f3, f4, f5, f6])\n",
    "maxBif = 6\n",
    "\n",
    "# Trigram features\n",
    "def trigram(word, index):\n",
    "    hindiSyllable = word[index]\n",
    "    f1 = 1.0\n",
    "    f2 = 1.0 if hindiSyllable in hVowels else 0.0\n",
    "    f3 = 1.0 if hindiSyllable in hConsonants else 0.0\n",
    "    f4 = 1.0 if hindiSyllable not in hConsonants else 0.0 # is the syllable a complete consonant?\n",
    "    f5 = 1.0 if u'\\u094d' in hindiSyllable else 0.0 # does the syllable contain a half consonant?\n",
    "    f6 = 1.0 if index > 0 else 0.0 # if index > 0, currY will also depend on prevX, prevY\n",
    "    f7 = 1.0 if index < len(word)-1 else 0 # currY will depend on both prevX,prevY and prev-1X, prev-1Y\n",
    "    \n",
    "    return np.array([f1, f2, f3, f4, f5, f6, f7])\n",
    "maxTrif = 7\n",
    "\n",
    "# Generates features with given feature model\n",
    "def features(tagPrev, word, index, ftype = \"uni\"):\n",
    "    \n",
    "    if ftype == \"uni\":\n",
    "        return unigram(tagPrev, word, index)\n",
    "    \n",
    "    if ftype == \"bi\":\n",
    "        return bigram(word, index)\n",
    "    \n",
    "    if ftype == \"tri\":\n",
    "        return trigram(word, index)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a feature & factor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0.]]\n",
      "[20.08553692 20.08553692  7.3890561 ]\n",
      "(36160,)\n"
     ]
    }
   ],
   "source": [
    "def word2features(word):\n",
    "#     return np.array([syllable2features(word, index) for index in range(len(word))])\n",
    "    return np.array([general_features(\"\",word, index, \"uni\") for index in wordLen(word)])\n",
    "\n",
    "TrainX = np.array([word2features(w) for w in trainX])\n",
    "\n",
    "\n",
    "def getFeatureMatrix(ftype = \"uni\"):\n",
    "    '''Creates a feature matrix over the whole data with given feature model'''\n",
    "    return np.array([\n",
    "    np.array([general_features(trainY[i][k-1], trainX[i], k)\n",
    "    for k in ks(trainX[i])])\n",
    "    for i in range(len(trainData))])\n",
    "\n",
    "featureMatrix = getFeatureMatrix()\n",
    "print(featureMatrix[0])\n",
    "\n",
    "def Fj(word):\n",
    "    '''F(x,y) feature statistics along the chain; returns an array indexed by feature_idx j'''\n",
    "    return np.sum([general_features(getEnglish(word)[index-1], getHindi(word), index, \"uni\") for index in ks(word)], axis=0)\n",
    "\n",
    "factorMatrix = []\n",
    "def getFactorMatrix(w):\n",
    "    '''Returns a factor matrix over the whole data with given weight vector'''\n",
    "    factMat = np.array([\n",
    "        np.array([general_factor(w,trainY[i][k-1], trainX[i], k)\n",
    "         for k in ks(trainX[i])])\n",
    "        for i in range(len(trainData))\n",
    "    ])\n",
    "    return factMat\n",
    "\n",
    "# Create weight vector according to feature model\n",
    "w0 = np.ones(maxUnif)\n",
    "\n",
    "factorMatrix = getFactorMatrix(w0)\n",
    "print(factorMatrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas/Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1]), 3: array([40.17107385, 40.17107385, 40.17107385, 40.17107385, 40.17107385,\n",
      "       40.17107385, 40.17107385, 40.17107385, 40.17107385, 40.17107385,\n",
      "       40.17107385, 40.17107385, 40.17107385, 40.17107385, 40.17107385,\n",
      "       40.17107385, 40.17107385, 40.17107385, 40.17107385, 40.17107385,\n",
      "       40.17107385, 40.17107385, 40.17107385, 40.17107385]), 2: array([593.65263641, 593.65263641, 593.65263641, 593.65263641,\n",
      "       593.65263641, 593.65263641, 593.65263641, 593.65263641,\n",
      "       593.65263641, 593.65263641, 593.65263641, 593.65263641,\n",
      "       593.65263641, 593.65263641, 593.65263641, 593.65263641,\n",
      "       593.65263641, 593.65263641, 593.65263641, 593.65263641,\n",
      "       593.65263641, 593.65263641, 593.65263641, 593.65263641]), 1: array([40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821,\n",
      "       40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821,\n",
      "       40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821,\n",
      "       40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821,\n",
      "       40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821,\n",
      "       40158.02847821, 40158.02847821, 40158.02847821, 40158.02847821]), 0: 963792.6834771498}\n"
     ]
    }
   ],
   "source": [
    "tagIndex = {tag: tagId for tagId, tag in enumerate(tags)}\n",
    "\n",
    "def alpha_trellis(w, word):\n",
    "    ''' returns a dictionary of forward variables {k : array of length num_states}'''    \n",
    "    alphas = {0: np.array([1 for id in tagIndex])}\n",
    "    factorMat = getFactorMatrix(w)\n",
    "    \n",
    "    for k in ks(word):\n",
    "        alphas[k] = np.einsum('i,j->i',alphas[k-1],factorMat[k-1])\n",
    "#         alphas[k] = np.array([\n",
    "#             np.sum([alphas[k-1][sPrevIdx] * general_factor(w, tagPrev,getHindi(word), k)\n",
    "#             for sPrev, sPrevIdx in tagIndex.items()])\n",
    "#             for tagPrev in tags\n",
    "#         ])\n",
    "    kmax = len(word)\n",
    "    alphas[kmax] = np.sum(alphas[kmax-1])\n",
    "    return alphas\n",
    "\n",
    "def beta_trellis(w, word):\n",
    "    ''' returns a dictionary of backward variables {k : array of length num_states}'''\n",
    "    kmax = len(word)\n",
    "    betas = {kmax: np.array([1 for tag in tagIndex])}\n",
    "    factorMat = getFactorMatrix(w)\n",
    "    for k in reversed(ks(word)):\n",
    "        betas[k] = np.einsum('i,j->i',betas[k+1],factorMat[k+1])\n",
    "#         betas[k] = np.array([\n",
    "#                 np.sum([ betas[k+1][sNextIdx] * general_factor(w, tagPrev, getHindi(word),k)\n",
    "#                     for sNext, sNextIdx in tagIndex.items()])\n",
    "#                 for tagPrev in tags])\n",
    "\n",
    "    betas[0] = np.sum(betas[1])\n",
    "    return betas\n",
    "\n",
    "w0 = np.ones(maxUnif)\n",
    "print(alpha_trellis(w0, trainData[0]))\n",
    "print(beta_trellis(w0, trainData[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Calculations & Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.68327258,  0.        , 22.93449075,  8.74878183,  0.        ,\n",
       "       25.95441513])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "\n",
    "w0 = np.ones(maxUnif)\n",
    "\n",
    "def fb_expectation_onseq(w, seq, alphas, betas):\n",
    "    '''part B of grad(w) for one sequence seq=(x,y) - returns a scalar'''\n",
    "    \n",
    "    return np.sum([\n",
    "        np.sum([\n",
    "            general_features(sPrev, getHindi(seq), k, ftype=\"uni\") *\n",
    "            alphas[k-1][sPrevIdx] * \n",
    "             general_factor(w, sPrev, getHindi(seq), k) *\n",
    "             betas[k+1][sNextIdx]\n",
    "        for sNext, sNextIdx in tagIndex.items()\n",
    "        for sPrev, sPrevIdx in tagIndex.items()], axis = 0)\n",
    "    for k in ks(seq)], axis = 0) / betas[0]\n",
    "\n",
    "alphas = alpha_trellis(w0, trainData[0])\n",
    "betas = beta_trellis(w0, trainData[0])\n",
    "\n",
    "fb_expect = fb_expectation_onseq(w0, trainData[0], alphas, betas)\n",
    "fb_expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_A(seqs):\n",
    "    '''part A (empirical) of grad(w) seqs=[(x1,y1), (x2,y2) ...]'''\n",
    "    return np.sum([Fj(s) for s in seqs])\n",
    "\n",
    "def gradient_B_oneseq(w,seq):\n",
    "    return fb_expectation_onseq(w, seq, alpha_trellis(w, seq), beta_trellis(w, seq))\n",
    "\n",
    "def gradient_B(w, seqs):\n",
    "    '''part B (expectation) of grad(w) seqs=[(x1,y1), (x2,y2) ...] '''\n",
    "    \n",
    "    # b is the sum of all the expectations\n",
    "    return np.sum([gradient_B_oneseq(w, s) for s in seqs], axis = 0)\n",
    "\n",
    "# def gradient_B_oneseq(w, seq):\n",
    "#     alphas = alpha_trellis(w, seq)\n",
    "#     betas = beta_trellis(w, seq)\n",
    "    \n",
    "#     features = featureMatrix\n",
    "#     factors = getFactorMatrix(w)\n",
    "#     i = 0\n",
    "#     return np.sum(np.array([\n",
    "#         np.einsum('i,i,j,k->i', alphas[k-1], betas[k+1], features[k][i], factors[k]) for k in ks(seq)\n",
    "#     ]), axis = 0)\n",
    "\n",
    "# def gradient_B(w, seqs):\n",
    "#     return np.sum([gradient_B_oneseq(w, seq) for seq in seqs])\n",
    "\n",
    "def gradient_C(w, sigmasq):\n",
    "    '''part C (regularizer) of grad(w)  '''\n",
    "    # c = w/sigma^2\n",
    "    return w / sigmasq\n",
    "\n",
    "\n",
    "def Z(w, seq):\n",
    "    # The first beta will be the last and final Z for the chain\n",
    "    return beta_trellis(w, seq)[0]\n",
    "\n",
    "\n",
    "def gradient(w, seqs, sigmasq):\n",
    "    ''' Gradient of the model at w. sigmasq is the inverse strength of the regularizer'''\n",
    "    return gradient_A(seqs) - gradient_B(w, seqs) - gradient_C(w, sigmasq)\n",
    "\n",
    "def log_likelihood(w, seqs, sigmasq):\n",
    "    '''Log-likelihood of the model at w. Sigmasq is the inverse strength of the regularizer'''\n",
    "    \n",
    "    # a is the sum of all the dot-products of w & fj(seq)\n",
    "    a = np.sum([np.dot(w, Fj(s)) for s in seqs])\n",
    "    \n",
    "    # b is the sum of log(Z) for every sequence\n",
    "    \n",
    "    b = np.sum([np.log(Z(w, s)) for s in seqs])\n",
    "    \n",
    "    # c is the sum of weight's square by twice the sigma square\n",
    "    c = np.sum(w**2 / 2 * sigmasq)\n",
    "    \n",
    "    return a - b - c\n",
    "\n",
    "# gradient_B(w0, trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([87742.,  4628., 28334., 59408.,  7642., 37001.]),\n",
       " array([46.46237269, 14.77910011, 22.93449075, 23.52788194,  0.        ,\n",
       "        38.06121056]),\n",
       " array([1., 1., 1., 1., 1., 1.]),\n",
       " 963792.6834771498)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_A(trainData) , gradient_B(w0, trainData[:2]), gradient_C(w0, 1.0), Z(w0,trainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divergence from gradient(w0) 48.907192549220575\n"
     ]
    }
   ],
   "source": [
    "# Self test!\n",
    "import scipy.optimize\n",
    "\n",
    "print ('divergence from gradient(w0)',scipy.optimize.check_grad(log_likelihood, gradient,  w0, trainData[0:2], 1.0))\n",
    "\n",
    "def neg_logl(w, seqs, sigmasq):\n",
    "    return - log_likelihood(w, seqs, sigmasq)\n",
    "\n",
    "def neg_grad(w, seqs, sigmasq):\n",
    "    return - gradient(w, seqs, sigmasq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_w(w, seqs, sigmasq, call_back=None):\n",
    "    return scipy.optimize.minimize( fun=neg_logl, x0=w, jac=neg_grad, args=(seqs, sigmasq), callback=call_back, method='TNC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 logl -13.170968622541302\n"
     ]
    }
   ],
   "source": [
    "# initial likelihood\n",
    "print ('w0 logl', log_likelihood(w0, trainData[:2], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.848019335915831 weights [-0.41066185  0.81103117  0.78062515  0.33570233  1.52698933  0.18878533]\n",
      "w_hat [-0.41066185  0.81103117  0.78062515  0.33570233  1.52698933  0.18878533]\n"
     ]
    }
   ],
   "source": [
    "# more self-tests\n",
    "\n",
    "def estim_debug(w):\n",
    "    print (log_likelihood(w, trainData[:2], 1.0), 'weights', w)\n",
    "\n",
    "\n",
    "res = estimate_w(w0, trainData[:2], 1.0 , estim_debug)\n",
    "w_hat = res.x\n",
    "\n",
    "print ('w_hat', w_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat logl -11.848019335915831\n",
      "w_hat grad [26.35455164 -1.01839324 11.30365041  5.57789055 -9.47301067 14.4197815 ]\n"
     ]
    }
   ],
   "source": [
    "# estimated likelihood\n",
    "print ('w_hat logl', log_likelihood(w_hat, trainData[0:2], 1.0))\n",
    "\n",
    "print ('w_hat grad', neg_grad(w_hat, trainData[0:2], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Predictions (Deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_trellis(w,seq):\n",
    "    ''' Compute the trellis with delta (max-scores) and phi (max-configurations) variables '''\n",
    "\n",
    "    deltas = {0: np.array([1.0 for s in tags])}\n",
    "    phis = {0: np.array([tagId for tag, tagId in tagIndex.items()], dtype = int)}\n",
    "    size = len(tags)\n",
    "#     scores = []\n",
    "    \n",
    "    factorMat = getFactorMatrix(w)\n",
    "    \n",
    "    for k in ks(seq):\n",
    "        # let initial deltas and phis of k be zeros\n",
    "        deltas[k] = np.zeros(size)\n",
    "        phis[k] = np.zeros(size, dtype = int)\n",
    "        \n",
    "        for sNext, sNextIdx in tagIndex.items():\n",
    "            \n",
    "#             sScore = np.array([\n",
    "#                 deltas[k-1][sPrevIdx] * factorMat[k]\n",
    "#                 for sPrev, sPrevIdx in tagIndex.items()\n",
    "#             ])\n",
    "\n",
    "            sScore = np.array([\n",
    "                deltas[k-1][sPrevIdx] * general_factor(w, sPrev, getHindi(seq), k)\n",
    "                for sPrev, sPrevIdx in tagIndex.items()\n",
    "            ])\n",
    "        \n",
    "            # get the maxScore\n",
    "            maxScore = np.argmax(sScore)\n",
    "\n",
    "            deltas[k][sNextIdx] = sScore[maxScore]\n",
    "            phis[k][sNextIdx] = maxScore\n",
    "\n",
    "    return deltas, phis\n",
    "\n",
    "def decode_trellis(seq, deltas, phis):\n",
    "    '''decodes the trellis and returns (y_hat, prob) where y_hat is the most likely sequence and prob is its unnormalized probability'''\n",
    "    \n",
    "    maxk = len(seq) - 1\n",
    "    endBest = np.argmax(deltas[maxk])\n",
    "    \n",
    "    yhat = np.zeros(len(seq), dtype = int)\n",
    "    yhat[maxk] = endBest\n",
    "    yhat_score = deltas[maxk][endBest]\n",
    "    \n",
    "    # go in reverse\n",
    "    for k in reversed(ks(seq)):\n",
    "        yhat[k-1] = phis[k][yhat[k]]\n",
    "        \n",
    "    return yhat, yhat_score\n",
    "\n",
    "\n",
    "def predict(w, seq):\n",
    "    ''' Predicts the most likely sequence and returns its (unnormalized) probability.'''\n",
    "\n",
    "    deltas, phis = delta_trellis(w, seq)\n",
    "    yhat, yhat_prob = decode_trellis(seq, deltas, phis)\n",
    "\n",
    "    return yhat, yhat_prob/Z(w,seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= [1. 1. 1. 1. 1. 1.]\n",
      "deltas\n",
      "      0          1           2            3\n",
      "0   1.0  20.085537  403.428793  8103.083928\n",
      "1   1.0  20.085537  403.428793  8103.083928\n",
      "2   1.0  20.085537  403.428793  8103.083928\n",
      "3   1.0  20.085537  403.428793  8103.083928\n",
      "4   1.0  20.085537  403.428793  8103.083928\n",
      "5   1.0  20.085537  403.428793  8103.083928\n",
      "6   1.0  20.085537  403.428793  8103.083928\n",
      "7   1.0  20.085537  403.428793  8103.083928\n",
      "8   1.0  20.085537  403.428793  8103.083928\n",
      "9   1.0  20.085537  403.428793  8103.083928\n",
      "10  1.0  20.085537  403.428793  8103.083928\n",
      "11  1.0  20.085537  403.428793  8103.083928\n",
      "12  1.0  20.085537  403.428793  8103.083928\n",
      "13  1.0  20.085537  403.428793  8103.083928\n",
      "14  1.0  20.085537  403.428793  8103.083928\n",
      "15  1.0  20.085537  403.428793  8103.083928\n",
      "16  1.0  20.085537  403.428793  8103.083928\n",
      "17  1.0  20.085537  403.428793  8103.083928\n",
      "18  1.0  20.085537  403.428793  8103.083928\n",
      "19  1.0  20.085537  403.428793  8103.083928\n",
      "20  1.0  20.085537  403.428793  8103.083928\n",
      "21  1.0  20.085537  403.428793  8103.083928\n",
      "22  1.0  20.085537  403.428793  8103.083928\n",
      "23  1.0  20.085537  403.428793  8103.083928\n",
      "\n",
      "phis\n",
      "     0  1  2  3\n",
      "0    0  1  1  1\n",
      "1    1  1  1  1\n",
      "2    2  1  1  1\n",
      "3    3  1  1  1\n",
      "4    4  1  1  1\n",
      "5    5  1  1  1\n",
      "6    6  1  1  1\n",
      "7    7  1  1  1\n",
      "8    8  1  1  1\n",
      "9    9  1  1  1\n",
      "10  10  1  1  1\n",
      "11  11  1  1  1\n",
      "12  12  1  1  1\n",
      "13  13  1  1  1\n",
      "14  14  1  1  1\n",
      "15  15  1  1  1\n",
      "16  16  1  1  1\n",
      "17  17  1  1  1\n",
      "18  18  1  1  1\n",
      "19  19  1  1  1\n",
      "20  20  1  1  1\n",
      "21  21  1  1  1\n",
      "22  22  1  1  1\n",
      "23  23  1  1  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_trellis():\n",
    "    deltas,phis = delta_trellis(w0, trainData[0])\n",
    "    print ('w=', w0)\n",
    "    print ('deltas')\n",
    "    print (pd.DataFrame(deltas))\n",
    "    print ('')\n",
    "    print ('phis')\n",
    "    print (pd.DataFrame(phis))\n",
    "\n",
    "    \n",
    "print_trellis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_rnd [ 0.58669579  0.78890075 -0.71493533  1.35097147  1.37760701  0.35548147]\n",
      "x_testseq ['अ' 'म' 'ज' 'द']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-017088822691>:6: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  matches = np.sum(y_gold == np.array(y_hat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yrnd [1 1 1 0] 0.0 0.0008624228156494865\n",
      "yone [1 1 1 0] 0.0 0.022853945845843236\n",
      "yhat [1 1 1 0] 0.0 0.009773437632299398\n",
      "y    ['a' 'ma' 'ja' 'x']\n"
     ]
    }
   ],
   "source": [
    "# compare performance on test data with w0 (one vector), w_hat (estimated) and w_rnd (random weights)\n",
    "\n",
    "w_rnd = np.random.randn(maxUnif)\n",
    "print ('w_rnd', w_rnd)\n",
    "\n",
    "print ('x_testseq', getHindi(testData[0]))\n",
    "y_rnd,p_rnd = predict(w_rnd, testData[0])\n",
    "y_one,p_one = predict(w0, testData[0])\n",
    "y_hat,p_hat = predict(w_hat, testData[0])\n",
    "\n",
    "\n",
    "y_gold = getEnglish(testData[0])\n",
    "print ('yrnd',np.array(y_rnd), elem_accuracy(y_rnd, y_gold ), p_rnd)\n",
    "print ('yone',np.array(y_one), elem_accuracy(y_one, y_gold ), p_one)\n",
    "print ('yhat',np.array(y_hat), elem_accuracy(y_hat, y_gold ), p_hat)\n",
    "print ('y   ', y_gold)\n",
    "\n",
    "print ('accuracy of w_one', np.average([ elem_accuracy(predict(w0, seq)[0], getEnglish(seq)) for seq in testData]))\n",
    "print ('accuracy of w_hat', np.average([ elem_accuracy(predict(w_hat, seq)[0], getEnglish(seq)) for seq in testData]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
